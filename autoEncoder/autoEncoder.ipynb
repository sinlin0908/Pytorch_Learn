{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"epoch\":100,\n",
    "    \"batch_size\":60,\n",
    "    \"LR\":0.001,\n",
    "    \"DOWNLOAD_MNIST\": False if os.path.exists('../mnist') else True,\n",
    "    \"N_TEST_IMG\":5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "不需要 test data\n",
    "'''\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='../mnist',\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=config[\"DOWNLOAD_MNIST\"],\n",
    ")\n",
    "\n",
    "data_loader = Data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(AutoEncoder,self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=28*28,\n",
    "                out_features=128,\n",
    "            ),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=128,\n",
    "                out_features=64,\n",
    "            ),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=64,\n",
    "                out_features=12,\n",
    "            ),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=12,\n",
    "                out_features=3,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=3,\n",
    "                out_features=12,\n",
    "            ),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=12,\n",
    "                out_features=64,\n",
    "            ),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=64,\n",
    "                out_features=128,\n",
    "            ),\n",
    "#             nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                in_features=128,\n",
    "                out_features=28*28,\n",
    "            ),\n",
    "            nn.Sigmoid(),  #因為原始資料為 0~1的數值\n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return encoded,decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=12, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=12, out_features=3, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=12, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=784, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss(loss_history):\n",
    "    \n",
    "    plt.plot(loss_history,label=\"loss\",linewidth=1)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim((0, 4))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "    \n",
    "def train(model,data_loader,view_data):\n",
    "    l_h = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=config['LR'])\n",
    "    loss_fun = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(config[\"epoch\"]):\n",
    "        for step ,(b_x,b_label) in enumerate(data_loader):\n",
    "            \n",
    "            # x y 都是從 x 來\n",
    "            b_x,b_y = b_x.view(-1,28*28).to(device) , b_x.view(-1,28*28).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            encoded,decoded = model(b_x)\n",
    "            \n",
    "            loss = loss_fun(decoded,b_y)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            l_h.append(loss.item())\n",
    "            \n",
    "            if step % 500 == 0:\n",
    "                \n",
    "                print('Epoch: ', epoch, '| step ',step,'| train loss: %.4f' % loss.item())\n",
    "                \n",
    "                pic = to_img(view_data.detach())\n",
    "                save_image(pic, './img2/image_source.png')\n",
    "                \n",
    "                _, out = model(view_data.to(device))\n",
    "                \n",
    "                pic = to_img(out.cpu().detach())\n",
    "                save_image(pic, './img2/image_{}_{}.png'.format(epoch,step))\n",
    "    \n",
    "    draw_loss(l_h)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | step  0 | train loss: 0.2326\n",
      "Epoch:  0 | step  500 | train loss: 0.0529\n",
      "Epoch:  1 | step  0 | train loss: 0.0424\n",
      "Epoch:  1 | step  500 | train loss: 0.0468\n",
      "Epoch:  2 | step  0 | train loss: 0.0417\n",
      "Epoch:  2 | step  500 | train loss: 0.0408\n",
      "Epoch:  3 | step  0 | train loss: 0.0371\n",
      "Epoch:  3 | step  500 | train loss: 0.0370\n",
      "Epoch:  4 | step  0 | train loss: 0.0362\n",
      "Epoch:  4 | step  500 | train loss: 0.0335\n",
      "Epoch:  5 | step  0 | train loss: 0.0333\n",
      "Epoch:  5 | step  500 | train loss: 0.0341\n",
      "Epoch:  6 | step  0 | train loss: 0.0338\n",
      "Epoch:  6 | step  500 | train loss: 0.0346\n",
      "Epoch:  7 | step  0 | train loss: 0.0307\n",
      "Epoch:  7 | step  500 | train loss: 0.0351\n",
      "Epoch:  8 | step  0 | train loss: 0.0352\n",
      "Epoch:  8 | step  500 | train loss: 0.0315\n",
      "Epoch:  9 | step  0 | train loss: 0.0337\n",
      "Epoch:  9 | step  500 | train loss: 0.0340\n",
      "Epoch:  10 | step  0 | train loss: 0.0354\n",
      "Epoch:  10 | step  500 | train loss: 0.0360\n",
      "Epoch:  11 | step  0 | train loss: 0.0324\n",
      "Epoch:  11 | step  500 | train loss: 0.0303\n",
      "Epoch:  12 | step  0 | train loss: 0.0314\n",
      "Epoch:  12 | step  500 | train loss: 0.0304\n",
      "Epoch:  13 | step  0 | train loss: 0.0315\n",
      "Epoch:  13 | step  500 | train loss: 0.0281\n",
      "Epoch:  14 | step  0 | train loss: 0.0321\n",
      "Epoch:  14 | step  500 | train loss: 0.0292\n",
      "Epoch:  15 | step  0 | train loss: 0.0325\n",
      "Epoch:  15 | step  500 | train loss: 0.0320\n",
      "Epoch:  16 | step  0 | train loss: 0.0320\n",
      "Epoch:  16 | step  500 | train loss: 0.0364\n",
      "Epoch:  17 | step  0 | train loss: 0.0363\n",
      "Epoch:  17 | step  500 | train loss: 0.0327\n",
      "Epoch:  18 | step  0 | train loss: 0.0303\n",
      "Epoch:  18 | step  500 | train loss: 0.0288\n",
      "Epoch:  19 | step  0 | train loss: 0.0290\n",
      "Epoch:  19 | step  500 | train loss: 0.0288\n",
      "Epoch:  20 | step  0 | train loss: 0.0328\n",
      "Epoch:  20 | step  500 | train loss: 0.0296\n",
      "Epoch:  21 | step  0 | train loss: 0.0322\n",
      "Epoch:  21 | step  500 | train loss: 0.0335\n",
      "Epoch:  22 | step  0 | train loss: 0.0314\n",
      "Epoch:  22 | step  500 | train loss: 0.0299\n",
      "Epoch:  23 | step  0 | train loss: 0.0352\n",
      "Epoch:  23 | step  500 | train loss: 0.0334\n",
      "Epoch:  24 | step  0 | train loss: 0.0291\n",
      "Epoch:  24 | step  500 | train loss: 0.0331\n",
      "Epoch:  25 | step  0 | train loss: 0.0312\n",
      "Epoch:  25 | step  500 | train loss: 0.0335\n",
      "Epoch:  26 | step  0 | train loss: 0.0276\n",
      "Epoch:  26 | step  500 | train loss: 0.0294\n",
      "Epoch:  27 | step  0 | train loss: 0.0309\n",
      "Epoch:  27 | step  500 | train loss: 0.0319\n",
      "Epoch:  28 | step  0 | train loss: 0.0301\n",
      "Epoch:  28 | step  500 | train loss: 0.0267\n",
      "Epoch:  29 | step  0 | train loss: 0.0314\n",
      "Epoch:  29 | step  500 | train loss: 0.0306\n",
      "Epoch:  30 | step  0 | train loss: 0.0336\n",
      "Epoch:  30 | step  500 | train loss: 0.0341\n",
      "Epoch:  31 | step  0 | train loss: 0.0306\n",
      "Epoch:  31 | step  500 | train loss: 0.0309\n",
      "Epoch:  32 | step  0 | train loss: 0.0316\n",
      "Epoch:  32 | step  500 | train loss: 0.0303\n",
      "Epoch:  33 | step  0 | train loss: 0.0325\n",
      "Epoch:  33 | step  500 | train loss: 0.0314\n",
      "Epoch:  34 | step  0 | train loss: 0.0292\n",
      "Epoch:  34 | step  500 | train loss: 0.0335\n",
      "Epoch:  35 | step  0 | train loss: 0.0283\n",
      "Epoch:  35 | step  500 | train loss: 0.0298\n",
      "Epoch:  36 | step  0 | train loss: 0.0289\n",
      "Epoch:  36 | step  500 | train loss: 0.0289\n",
      "Epoch:  37 | step  0 | train loss: 0.0287\n",
      "Epoch:  37 | step  500 | train loss: 0.0314\n",
      "Epoch:  38 | step  0 | train loss: 0.0336\n",
      "Epoch:  38 | step  500 | train loss: 0.0251\n",
      "Epoch:  39 | step  0 | train loss: 0.0313\n",
      "Epoch:  39 | step  500 | train loss: 0.0300\n",
      "Epoch:  40 | step  0 | train loss: 0.0298\n",
      "Epoch:  40 | step  500 | train loss: 0.0299\n",
      "Epoch:  41 | step  0 | train loss: 0.0337\n",
      "Epoch:  41 | step  500 | train loss: 0.0297\n",
      "Epoch:  42 | step  0 | train loss: 0.0291\n",
      "Epoch:  42 | step  500 | train loss: 0.0350\n",
      "Epoch:  43 | step  0 | train loss: 0.0307\n",
      "Epoch:  43 | step  500 | train loss: 0.0284\n",
      "Epoch:  44 | step  0 | train loss: 0.0301\n",
      "Epoch:  44 | step  500 | train loss: 0.0296\n",
      "Epoch:  45 | step  0 | train loss: 0.0308\n",
      "Epoch:  45 | step  500 | train loss: 0.0320\n",
      "Epoch:  46 | step  0 | train loss: 0.0289\n",
      "Epoch:  46 | step  500 | train loss: 0.0323\n",
      "Epoch:  47 | step  0 | train loss: 0.0288\n",
      "Epoch:  47 | step  500 | train loss: 0.0277\n",
      "Epoch:  48 | step  0 | train loss: 0.0308\n",
      "Epoch:  48 | step  500 | train loss: 0.0311\n",
      "Epoch:  49 | step  0 | train loss: 0.0317\n",
      "Epoch:  49 | step  500 | train loss: 0.0306\n",
      "Epoch:  50 | step  0 | train loss: 0.0327\n",
      "Epoch:  50 | step  500 | train loss: 0.0281\n",
      "Epoch:  51 | step  0 | train loss: 0.0323\n",
      "Epoch:  51 | step  500 | train loss: 0.0300\n",
      "Epoch:  52 | step  0 | train loss: 0.0289\n",
      "Epoch:  52 | step  500 | train loss: 0.0274\n",
      "Epoch:  53 | step  0 | train loss: 0.0298\n",
      "Epoch:  53 | step  500 | train loss: 0.0323\n",
      "Epoch:  54 | step  0 | train loss: 0.0313\n",
      "Epoch:  54 | step  500 | train loss: 0.0291\n",
      "Epoch:  55 | step  0 | train loss: 0.0263\n",
      "Epoch:  55 | step  500 | train loss: 0.0271\n",
      "Epoch:  56 | step  0 | train loss: 0.0295\n",
      "Epoch:  56 | step  500 | train loss: 0.0290\n",
      "Epoch:  57 | step  0 | train loss: 0.0323\n",
      "Epoch:  57 | step  500 | train loss: 0.0305\n",
      "Epoch:  58 | step  0 | train loss: 0.0261\n",
      "Epoch:  58 | step  500 | train loss: 0.0287\n",
      "Epoch:  59 | step  0 | train loss: 0.0332\n",
      "Epoch:  59 | step  500 | train loss: 0.0281\n",
      "Epoch:  60 | step  0 | train loss: 0.0270\n",
      "Epoch:  60 | step  500 | train loss: 0.0277\n",
      "Epoch:  61 | step  0 | train loss: 0.0291\n",
      "Epoch:  61 | step  500 | train loss: 0.0271\n",
      "Epoch:  62 | step  0 | train loss: 0.0278\n",
      "Epoch:  62 | step  500 | train loss: 0.0317\n",
      "Epoch:  63 | step  0 | train loss: 0.0311\n",
      "Epoch:  63 | step  500 | train loss: 0.0323\n",
      "Epoch:  64 | step  0 | train loss: 0.0277\n",
      "Epoch:  64 | step  500 | train loss: 0.0273\n",
      "Epoch:  65 | step  0 | train loss: 0.0294\n",
      "Epoch:  65 | step  500 | train loss: 0.0303\n",
      "Epoch:  66 | step  0 | train loss: 0.0311\n",
      "Epoch:  66 | step  500 | train loss: 0.0302\n",
      "Epoch:  67 | step  0 | train loss: 0.0269\n",
      "Epoch:  67 | step  500 | train loss: 0.0293\n",
      "Epoch:  68 | step  0 | train loss: 0.0287\n",
      "Epoch:  68 | step  500 | train loss: 0.0276\n",
      "Epoch:  69 | step  0 | train loss: 0.0323\n",
      "Epoch:  69 | step  500 | train loss: 0.0283\n",
      "Epoch:  70 | step  0 | train loss: 0.0327\n",
      "Epoch:  70 | step  500 | train loss: 0.0282\n",
      "Epoch:  71 | step  0 | train loss: 0.0282\n",
      "Epoch:  71 | step  500 | train loss: 0.0301\n",
      "Epoch:  72 | step  0 | train loss: 0.0301\n",
      "Epoch:  72 | step  500 | train loss: 0.0286\n",
      "Epoch:  73 | step  0 | train loss: 0.0306\n",
      "Epoch:  73 | step  500 | train loss: 0.0312\n",
      "Epoch:  74 | step  0 | train loss: 0.0277\n",
      "Epoch:  74 | step  500 | train loss: 0.0287\n",
      "Epoch:  75 | step  0 | train loss: 0.0290\n",
      "Epoch:  75 | step  500 | train loss: 0.0268\n",
      "Epoch:  76 | step  0 | train loss: 0.0302\n",
      "Epoch:  76 | step  500 | train loss: 0.0321\n",
      "Epoch:  77 | step  0 | train loss: 0.0307\n",
      "Epoch:  77 | step  500 | train loss: 0.0273\n",
      "Epoch:  78 | step  0 | train loss: 0.0261\n",
      "Epoch:  78 | step  500 | train loss: 0.0273\n",
      "Epoch:  79 | step  0 | train loss: 0.0305\n",
      "Epoch:  79 | step  500 | train loss: 0.0310\n",
      "Epoch:  80 | step  0 | train loss: 0.0299\n",
      "Epoch:  80 | step  500 | train loss: 0.0279\n",
      "Epoch:  81 | step  0 | train loss: 0.0307\n",
      "Epoch:  81 | step  500 | train loss: 0.0298\n",
      "Epoch:  82 | step  0 | train loss: 0.0277\n",
      "Epoch:  82 | step  500 | train loss: 0.0288\n",
      "Epoch:  83 | step  0 | train loss: 0.0290\n",
      "Epoch:  83 | step  500 | train loss: 0.0280\n",
      "Epoch:  84 | step  0 | train loss: 0.0272\n",
      "Epoch:  84 | step  500 | train loss: 0.0272\n",
      "Epoch:  85 | step  0 | train loss: 0.0291\n",
      "Epoch:  85 | step  500 | train loss: 0.0298\n",
      "Epoch:  86 | step  0 | train loss: 0.0328\n",
      "Epoch:  86 | step  500 | train loss: 0.0274\n",
      "Epoch:  87 | step  0 | train loss: 0.0287\n",
      "Epoch:  87 | step  500 | train loss: 0.0325\n",
      "Epoch:  88 | step  0 | train loss: 0.0295\n",
      "Epoch:  88 | step  500 | train loss: 0.0320\n",
      "Epoch:  89 | step  0 | train loss: 0.0275\n",
      "Epoch:  89 | step  500 | train loss: 0.0314\n",
      "Epoch:  90 | step  0 | train loss: 0.0284\n",
      "Epoch:  90 | step  500 | train loss: 0.0254\n",
      "Epoch:  91 | step  0 | train loss: 0.0296\n",
      "Epoch:  91 | step  500 | train loss: 0.0255\n",
      "Epoch:  92 | step  0 | train loss: 0.0309\n",
      "Epoch:  92 | step  500 | train loss: 0.0276\n",
      "Epoch:  93 | step  0 | train loss: 0.0276\n",
      "Epoch:  93 | step  500 | train loss: 0.0311\n",
      "Epoch:  94 | step  0 | train loss: 0.0290\n",
      "Epoch:  94 | step  500 | train loss: 0.0324\n",
      "Epoch:  95 | step  0 | train loss: 0.0255\n",
      "Epoch:  95 | step  500 | train loss: 0.0323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  96 | step  0 | train loss: 0.0274\n",
      "Epoch:  96 | step  500 | train loss: 0.0297\n",
      "Epoch:  97 | step  0 | train loss: 0.0259\n",
      "Epoch:  97 | step  500 | train loss: 0.0312\n",
      "Epoch:  98 | step  0 | train loss: 0.0310\n",
      "Epoch:  98 | step  500 | train loss: 0.0290\n",
      "Epoch:  99 | step  0 | train loss: 0.0324\n",
      "Epoch:  99 | step  500 | train loss: 0.0286\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_data = train_data.train_data[:config['N_TEST_IMG']].view(-1, 28*28).type(torch.FloatTensor)/255.\n",
    "\n",
    "train(model,data_loader,view_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(model,test_data):\n",
    "    model.eval()\n",
    "    pic = to_img(test_data.detach())       \n",
    "    save_image(pic, './img2/test_source.png')\n",
    "                \n",
    "    _, out = model(test_data.to(device))\n",
    "\n",
    "    pic = to_img(out.cpu().detach())    \n",
    "    save_image(pic, './img2/test.png')\n",
    "    \n",
    "\n",
    "        \n",
    "test_data = train_data.train_data[-config['N_TEST_IMG']:].view(-1, 28*28).type(torch.FloatTensor)/255.        \n",
    "\n",
    "test(model,test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
